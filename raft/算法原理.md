# Raft 选主算法原理详解

## 一、为什么需要选主？

在分布式系统中，多个节点需要协调一致地工作。为了避免冲突和保证一致性，通常需要选出一个 Leader 来：
- 处理客户端请求
- 管理日志复制
- 做出决策

## 二、Raft 选主的核心机制

### 1. 节点状态转换

```
FOLLOWER → CANDIDATE → LEADER
    ↑         ↓           ↓
    └─────────┴───────────┘
```

- **FOLLOWER（跟随者）**：默认状态，被动接收 Leader 的心跳
- **CANDIDATE（候选者）**：选举超时后发起选举
- **LEADER（领导者）**：获得大多数选票后成为 Leader

### 2. 选举超时机制

**关键点：随机超时时间**

```java
int timeout = 150 + random.nextInt(150); // 150-300ms
```

**为什么需要随机？**
- 如果所有节点同时超时，会同时成为 Candidate
- 可能导致"分票"（split vote），无法选出 Leader
- 随机化让某个节点更可能先超时，先发起选举

### 3. 选举过程

#### 步骤 1：触发选举
- Follower 在选举超时时间内没有收到 Leader 的心跳
- 转换为 Candidate，任期号 +1

#### 步骤 2：发起投票
- 投票给自己
- 向所有其他节点发送 `RequestVote` RPC

#### 步骤 3：收集选票
- 如果收到大多数选票（> n/2），成为 Leader
- 如果收到更高任期的消息，转换为 Follower
- 如果选举超时，重新发起选举

### 4. 投票规则

节点会投票给候选者，当且仅当：

1. **任期号检查**：候选者的任期号 >= 当前节点的任期号
2. **投票状态检查**：当前节点还没有投票给其他候选者（或已经投票给该候选者）
3. **日志新旧检查**：候选者的日志至少和当前节点一样新

**日志新旧比较规则：**
```java
// 比较最后一条日志的任期号
if (candidateLastLogTerm > myLastLogTerm) {
    return true; // 候选者日志更新
}
if (candidateLastLogTerm == myLastLogTerm && 
    candidateLastLogIndex >= myLastLogIndex) {
    return true; // 任期相同，但候选者日志更长或相等
}
return false; // 候选者日志更旧
```

**为什么需要比较日志新旧？**
- 确保 Leader 包含所有已提交的日志条目
- 防止数据丢失和不一致

### 5. 心跳机制

Leader 定期（50ms）向所有 Follower 发送心跳：

```java
AppendEntriesRequest request = new AppendEntriesRequest(
    term, leaderId, prevLogIndex, prevLogTerm, null, commitIndex
);
```

**心跳的作用：**
- 告诉 Follower：我还活着，仍然是 Leader
- Follower 收到心跳后重置选举超时定时器
- 如果 Follower 长时间没收到心跳，会触发新的选举

## 三、关键设计原则

### 1. 大多数原则（Majority）

**定义**：需要获得超过半数的选票才能成为 Leader

```java
if (voteCount > allNodeIds.size() / 2) {
    becomeLeader();
}
```

**为什么需要大多数？**
- **防止脑裂**：确保同一任期内最多只有一个 Leader
- **网络分区容错**：即使发生网络分区，也最多只有一个分区能选出 Leader

**示例**：5 个节点的集群
- 需要至少 3 票才能成为 Leader
- 即使网络分区成 2+3，也只有 3 个节点的分区能选出 Leader

### 2. 任期号（Term）

**定义**：单调递增的整数，标识 Leader 的任期

**作用：**
- 区分不同的 Leader 任期
- 检测过期的消息和 Leader
- 确保选举的正确性

**规则：**
- 每次发起选举，任期号 +1
- 收到更高任期的消息，更新任期并转换为 Follower
- 同一任期内最多只有一个 Leader

### 3. 状态持久化

**持久化状态**（需要保存到磁盘）：
- `currentTerm`：当前任期号
- `votedFor`：投票给哪个候选者
- `log[]`：日志条目

**为什么需要持久化？**
- 节点重启后需要恢复状态
- 防止重复投票
- 保证一致性

## 四、选举场景分析

### 场景 1：正常选举

```
时刻 T0: 所有节点都是 Follower
时刻 T1: 节点 1 超时，成为 Candidate，发起选举
时刻 T2: 节点 1 获得 3/5 票，成为 Leader
时刻 T3: Leader 开始发送心跳，其他节点保持 Follower
```

### 场景 2：Leader 宕机

```
时刻 T0: 节点 1 是 Leader，定期发送心跳
时刻 T1: 节点 1 宕机，停止发送心跳
时刻 T2: 节点 2 超时，成为 Candidate，发起选举
时刻 T3: 节点 2 获得 3/5 票，成为新 Leader
```

### 场景 3：分票（Split Vote）

```
时刻 T0: 所有节点都是 Follower
时刻 T1: 节点 1 和节点 2 同时超时，都成为 Candidate
时刻 T2: 节点 1 获得 2 票，节点 2 获得 2 票，都没有超过半数
时刻 T3: 选举超时，重新选举
时刻 T4: 由于随机超时，某个节点先超时，成功当选
```

### 场景 4：网络分区

```
集群：5 个节点
分区 A：节点 1, 2（2 个节点）
分区 B：节点 3, 4, 5（3 个节点）

分区 A：无法选出 Leader（需要 3 票，只有 2 个节点）
分区 B：可以选出 Leader（3 个节点，可以获得 2 票）
```

## 五、实现细节

### 1. 线程安全

- 使用 `ReentrantLock` 保护状态转换
- 使用 `AtomicLong` 管理任期号
- 使用 `CopyOnWriteArrayList` 存储日志

### 2. 定时器管理

```java
// 选举超时定时器
electionTimer = scheduler.schedule(() -> {
    if (state == NodeState.FOLLOWER) {
        becomeCandidate();
    }
}, randomTimeout, TimeUnit.MILLISECONDS);

// 心跳定时器（仅 Leader）
heartbeatTimer = scheduler.scheduleAtFixedRate(
    this::sendHeartbeat,
    HEARTBEAT_INTERVAL,
    HEARTBEAT_INTERVAL,
    TimeUnit.MILLISECONDS
);
```

### 3. 状态转换的原子性

所有状态转换都在锁保护下进行，确保：
- 状态一致性
- 避免竞态条件
- 正确更新相关变量

## 六、性能考虑

### 1. 选举超时时间

- **太短**：网络延迟可能导致频繁选举
- **太长**：Leader 宕机后恢复时间过长
- **推荐**：150-300ms（根据网络环境调整）

### 2. 心跳间隔

- **太短**：增加网络负载
- **太长**：Follower 可能误判 Leader 宕机
- **推荐**：50ms（通常远小于选举超时）

### 3. 大多数原则

- 5 节点集群：需要 3 票
- 可以容忍 2 个节点故障
- 3 节点集群：需要 2 票
- 可以容忍 1 个节点故障

## 七、常见问题

### Q1: 为什么需要随机超时？
**A**: 避免多个节点同时发起选举，导致分票。

### Q2: 为什么需要大多数选票？
**A**: 防止出现多个 Leader（脑裂），保证一致性。

### Q3: 为什么需要比较日志新旧？
**A**: 确保 Leader 包含所有已提交的日志，防止数据丢失。

### Q4: 如果所有节点同时超时怎么办？
**A**: 随机超时让这种情况概率很低。即使发生，选举超时后会重新选举。

### Q5: 网络分区时会发生什么？
**A**: 只有包含大多数节点的分区能选出 Leader。少数节点的分区无法选出 Leader，无法处理写请求。

## 八、与 Paxos 的对比

| 特性 | Raft | Paxos |
|------|------|-------|
| 理解难度 | 相对简单 | 较复杂 |
| Leader | 必须有 Leader | 不需要固定 Leader |
| 选主 | 明确的选主算法 | 隐含在一致性算法中 |
| 日志复制 | 顺序复制 | 可以并行 |
| 实际应用 | 广泛使用（etcd, Consul） | 理论更完善 |

## 九、实际应用

- **etcd**：Kubernetes 的键值存储
- **Consul**：服务发现和配置管理
- **TiKV**：分布式数据库
- **CockroachDB**：分布式 SQL 数据库

